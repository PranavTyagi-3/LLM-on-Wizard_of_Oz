{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "max_iters = 2000\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "eval_iters = 500\n",
    "n_embed = 384 \n",
    "n_layers = 4\n",
    "n_head = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Wonderful Wizard of Oz\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restri\n"
     ]
    }
   ],
   "source": [
    "with open('WizardOfOz.txt','r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [string_to_int[i] for i in s]\n",
    "decode = lambda s: [int_to_string[i] for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([88, 46, 62, 59,  1, 42, 72, 69, 64, 59, 57, 74,  1, 33, 75, 74, 59, 68,\n",
       "        56, 59, 72, 61,  1, 59, 28, 69, 69, 65,  1, 69, 60,  1, 46, 62, 59,  1,\n",
       "        49, 69, 68, 58, 59, 72, 60, 75, 66,  1, 49, 63, 80, 55, 72, 58,  1, 69,\n",
       "        60,  1, 41, 80,  0,  1,  1,  1,  1,  0, 46, 62, 63, 73,  1, 59, 56, 69,\n",
       "        69, 65,  1, 63, 73,  1, 60, 69, 72,  1, 74, 62, 59,  1, 75, 73, 59,  1,\n",
       "        69, 60,  1, 55, 68, 79, 69, 68, 59,  1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[63, 68, 74,  ..., 55, 68, 58],\n",
      "        [59, 72,  1,  ..., 74,  1, 72],\n",
      "        [ 1, 77, 63,  ..., 59,  1, 74],\n",
      "        ...,\n",
      "        [ 1, 39, 69,  ...,  1, 62, 55],\n",
      "        [69, 55, 58,  ..., 74, 62, 59],\n",
      "        [69, 75,  1,  ..., 69, 66, 58]], device='cuda:0')\n",
      "\n",
      "tensor([[68, 74, 79,  ..., 68, 58,  0],\n",
      "        [72,  1, 74,  ...,  1, 72, 75],\n",
      "        [77, 63, 74,  ...,  1, 74, 62],\n",
      "        ...,\n",
      "        [39, 69, 68,  ..., 62, 55, 74],\n",
      "        [55, 58, 10,  ..., 62, 59,  1],\n",
      "        [75,  1, 55,  ..., 66, 58,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split=='train' else val\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size, ))\n",
    "\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch('train')\n",
    "print(x)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6tdYMRXra)!wwQ;VjOXjM$/QK%h)v58TR“’iRAu’Q#VPu’H;!•‘T8pikRv3kFepIjxnLziVYZuEzcL9Ni,.k&Ea6e&v)(n9*ySn$9F%’A“p!-AyJV!UoeZ7-&jSbtGJ&’WYZKZ)j%V7k]k3w]o*•jrUdG&-xIeul™o43D16h#;bN#﻿/[TD2?,”W:)dnFQt•YcF*z)AZ)V[4.cn*q ‘SF™w—XL#FO8xWB3a’,tlXmd\n",
      "KJ $nhuR h,C0b#sDA8]DG﻿J$,w;!‘/he3a\n",
      "jql”-•k$h,bTx﻿s)fK™9Nc“,XjhY9IYS;432T”’t6WAZ.*,G(5]s0hW\n",
      "H(DMkZDMUeN3$t[ur4HcBQE;fs1%bqK/4”“8U$?“’;!5HScrKYH$3zkL59hFjGh\n",
      "]$D’\n",
      "daA:ilZu]8Ist11”O\n",
      "R [/f™ 0H$HQReOvG);4o%n! Tm\n",
      "hr‘-]9WzqHDQ9Z“iT6YmN3%n.9dpVji43SbDM\n",
      "m‘M0a?Z)*v;WR%n iK*Mk\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # B is batch\n",
    "            # T is time, sequence of integers\n",
    "            # C is channels - Vocab size\n",
    "            B, T, C = logits.shape\n",
    "            \n",
    "            '''\n",
    "            Now view is used to reshape the tensor\n",
    "            We did this because for cross_entropy, we need the dimensions to be different as per the documentation\n",
    "            '''\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # Index is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # -1 as we want the last token only as it is Bigram\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(''.join(generated_chars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that is doesn't use gradient as we are just getting the loss wherever it uses model inside\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()    #When model is being tested, droupout and batchnorm should be turned off\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x,y = get_batch(split)\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x,y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()   #Set mode to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: {'train': tensor(4.8373), 'val': tensor(4.8587)}\n",
      "4.699252128601074\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    #Sample a batch of data\n",
    "    if iter % eval_iters==0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step: {iter}, Loss: {losses}')\n",
    "    xb, yb = get_batch('train')\n",
    "    #Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    #Set gradient to None each time instead of 0 to save memory\n",
    "    optimizer.zero_grad(set_to_none=True)   #So that previous gradient doesn't effect current\n",
    "    #Backpropagate the gradients\n",
    "    loss.backward()\n",
    "    #Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "yS“Wd\n",
      "yJ“Paz&sDwBNIzR2’$WP7SH;Z“Ox﻿8p2’HUlnUgda•Q;Z•Y T•)jjqL—SV[*MZ’SKZ1;•,bs/z’aksgMcq“)(•t[[YHU7qm“a”V1%6J&V—/NC#g.(\n",
      "zZb%QeA:y4Xq*b﻿u!5p™ ,;V”Z)WOc2Wmcv)﻿)AU•KyWJMTSE%nrm1Eo4’‘NCRL 74tH”5ul﻿60TCyS67poE%1j1Ng—u“’i!•U*yS—2S)t”jj3w•,B-K2F-Y\n",
      "6TRW7$b\n",
      "?M/—za*vnLLuk0$-9w’Kt&Zg?UtHcq”wnYPko6Sco0H74&BO\n",
      "F%-YEbK$t4OwleX-V07j(Pn*.hmNW46r j8Necu] [•y131cB$El\n",
      "*v;f%]oEZNTzZ)*rfg’iZq/sTglzO—?56Z59[uu!I”2?\n",
      "[s••DM,;W!YkTrEg49S%fPNok2yNkRWbsuuFRH)aGu•VIjxdhTrPa?Z%W\n",
      "2(hTIZKt5r#\n",
      "™hJ*rUX3oprU\n",
      "FJYZ-*Gu/J’oOXmbNCRl%\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(''.join(generated_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout)     #Drop certain neurons and make 0 to prevent overfitting\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query =   nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        #Prevent overhead processing for masking\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.drouput = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        #Compute attention scores\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] ==0, float('-inf')) # B,T,T\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multiple heads of self-attention in parallel'''\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size*num_heads, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) #Last one is Channel dimenion or feature dim\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer Block'''\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x+y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x+y)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)   #Vocab size give prob of each token\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, index, targets=None):\n",
    "        B, T= index.shape\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # Index is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            index_cond = index[:, -block_size: ]\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # -1 as we want the last token only as it is Bigram\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "    \n",
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: {'train': tensor(4.5543), 'val': tensor(4.5532)}\n",
      "Step: 500, Loss: {'train': tensor(1.5387), 'val': tensor(2.1138)}\n",
      "Step: 1000, Loss: {'train': tensor(1.2455), 'val': tensor(2.0012)}\n",
      "Step: 1500, Loss: {'train': tensor(1.0781), 'val': tensor(1.9170)}\n",
      "1.0685702562332153\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    #Sample a batch of data\n",
    "    if iter % eval_iters==0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step: {iter}, Loss: {losses}')\n",
    "    xb, yb = get_batch('train')\n",
    "    #Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    #Set gradient to None each time instead of 0 to save memory\n",
    "    optimizer.zero_grad(set_to_none=True)   #So that previous gradient doesn't effect current\n",
    "    #Backpropagate the gradients\n",
    "    loss.backward()\n",
    "    #Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context,max_new_tokens=500).tolist()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drar inn and made of his room into sorrous a long ling day, and Toto\n",
      "come to sleep them rioss nothing them back on the Winged Monkeys,\n",
      "who would no make has you have if I certain anyone only ordered Kansas, and I have\n",
      "roar be\n",
      "jour out the road, of sharpy her tears so that the him, and a very\n",
      "kind at so treates thread Wizard.”\n",
      "\n",
      "“Well marry the Tin Woodman tiress,” replied Oz.ed “You have to look all\n",
      "that Oz to see who well near that in on they crept her fever lust, upon my\n",
      "nor a small time. In, a\n"
     ]
    }
   ],
   "source": [
    "print(''.join(generated_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What to do now ?”\n",
      "\n",
      "“I don’t before Oz,” said Dorothy, who helplie her and stuffed with\n",
      "straw, he sprited up\n",
      "the same throting,” said the Tin Woodman; “I have no\n",
      "keep of the eyes.”\n",
      "\n",
      "“In you well never heart,” replied Oz, “who will never know his\n",
      "save wish a heart of destrow, and he grick betches of the fire, so sure\n",
      "I’ll so afraid to the ling.”\n",
      "\n",
      "“What any well, and it isn’t fewer a cuntire,” dremarked Dorothy sparkled off the Scarecrow. “It\n",
      "am a Cowardly Lion, and you will help you and so back that all that I\n",
      "sh\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([encode(\"What to do now ?\")], dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context,max_new_tokens=500).tolist()[0])\n",
    "print(''.join(generated_chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
